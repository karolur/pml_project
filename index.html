<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Pml project by karolur</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Pml project</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/karolur/pml_project" class="btn">View on GitHub</a>
      <a href="https://github.com/karolur/pml_project/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/karolur/pml_project/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p></p>Practical machine learning project

<p></p>


.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}


<div>


<div id="header">
<h1>
<a id="practical-machine-learning-project" class="anchor" href="#practical-machine-learning-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Practical machine learning project</h1>
<h4>
<a id="karol" class="anchor" href="#karol" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>Karol</em>
</h4>
</div>

<div id="introduction">
<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>
<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset). The goal of the project is to predict the manner in which they did their exercises.</p>
</div>

<div id="getting-and-cleaning-data">
<h2>
<a id="getting-and-cleaning-data" class="anchor" href="#getting-and-cleaning-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting and cleaning data</h2>
<pre><code>library(reshape2)
library(plyr); library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: 'dplyr'
## 
## The following objects are masked from 'package:plyr':
## 
##     arrange, count, desc, failwith, id, mutate, rename, summarise,
##     summarize
## 
## The following object is masked from 'package:stats':
## 
##     filter
## 
## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<pre><code>#URL &lt;- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#download.file(URL, destfile = "./train_data.csv", method="curl")
train &lt;- read.csv("train_data.csv")
#URL &lt;- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download.file(URL, destfile = "./test_data.csv", method="curl")
test &lt;- read.csv("test_data.csv")</code></pre>
<p>check the general structure of both datasets</p>
<pre><code>str(train)
str(test)
nrow(train)
nrow(test)</code></pre>
<p>Because there are some ’’ values, the datasets were re-read. A list of the number of NA per each column was also produced to see if we could eliminate some features, additionally all the columns related to timestamps were deleted as these will not be part of the model.</p>
<pre><code>train &lt;- read.csv("train_data.csv", na.strings=c("NA","", " "))
test &lt;- read.csv("test_data.csv", na.strings=c("NA","", " "))

train &lt;- train[, -c(1:7)]

na_count_train &lt;- sapply(train, function(y) sum(is.na(y)))
na_count_test &lt;- sapply(test, function(y) sum(is.na(y)))</code></pre>
<p>Since no columns have only NA, near zero variance was used converting all values to numeric.</p>
<pre><code>nzv &lt;- nearZeroVar(train, saveMetrics=TRUE)
train &lt;- train[, !nzv$nzv]
isnaCol &lt;- sapply(train, function(y) sum(is.na(y))==0)
train &lt;- train[, isnaCol]

asNumeric &lt;- function(x) as.numeric(as.character(x))
factorsNumeric &lt;- function(d) {factorCol &lt;- as.vector(sapply(d, is.factor))
                              factorCol[length(factorCol)] = FALSE
                              modifyList(d, lapply(d[, factorCol],   
                                                   asNumeric))}
train &lt;- factorsNumeric(train)
features &lt;- names(train[,-length(train)])
test &lt;- test[,features]</code></pre>
<pre><code>## 
##    A    B    C    D    E 
## 5580 3797 3422 3216 3607</code></pre>
<p>following <a href="http://machinelearningmastery.com/feature-selection-with-the-caret-r-package/">this guide</a> on feature selection and random forest.</p>
<p>Caret uses recursive feature elimination to select the features and build the model. Random forest was chosen.The algorithm fits the model to all predictors and each predictor is ranked using it’s importance to the model, then for each top ranked predictors the model is refit and assessted until the better performer is retained. Resampling with cross validation is used to incorporate the variation due to the selection at each iteration. A 10 fold cv is used with random forest (rfFuncs)</p>
<pre><code>set.seed(666)
inTrain = createDataPartition(train$classe, p = 0.65, list = FALSE)
train2 = train[inTrain,]
test2 = train[-inTrain,]


# define the control using a random forest selection function
control &lt;- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results &lt;- rfe(train2[,1:52], train2[,53], sizes=c(1:52), rfeControl=control)</code></pre>
<pre><code>## Loading required package: randomForest
## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.
## 
## Attaching package: 'randomForest'
## 
## The following object is masked from 'package:dplyr':
## 
##     combine</code></pre>
<pre><code># summarize the results
#print(results)
# list the chosen features
predictors(results)</code></pre>
<pre><code>##  [1] "roll_belt"            "yaw_belt"             "magnet_dumbbell_z"   
##  [4] "pitch_belt"           "magnet_dumbbell_y"    "pitch_forearm"       
##  [7] "accel_dumbbell_y"     "roll_forearm"         "roll_dumbbell"       
## [10] "magnet_dumbbell_x"    "accel_dumbbell_z"     "magnet_forearm_z"    
## [13] "roll_arm"             "gyros_belt_z"         "magnet_belt_z"       
## [16] "magnet_belt_y"        "accel_forearm_x"      "yaw_arm"             
## [19] "magnet_belt_x"        "yaw_dumbbell"         "magnet_forearm_y"    
## [22] "gyros_arm_y"          "accel_forearm_z"      "gyros_dumbbell_y"    
## [25] "accel_belt_z"         "total_accel_dumbbell" "gyros_forearm_y"     
## [28] "magnet_arm_z"         "yaw_forearm"          "gyros_dumbbell_x"    
## [31] "accel_forearm_y"      "gyros_arm_x"          "accel_dumbbell_x"    
## [34] "accel_arm_y"          "gyros_forearm_z"      "magnet_forearm_x"    
## [37] "total_accel_forearm"  "pitch_arm"            "accel_arm_z"         
## [40] "gyros_dumbbell_z"     "accel_arm_x"          "magnet_arm_x"        
## [43] "total_accel_arm"      "gyros_belt_x"         "magnet_arm_y"        
## [46] "accel_belt_x"</code></pre>
<pre><code>#confusion matrix 
print(results$fit)</code></pre>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, importance = first) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 6
## 
##         OOB estimate of  error rate: 0.5%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3621    5    0    0    1 0.001654260
## B    9 2456    4    0    0 0.005265290
## C    0   15 2207    3    0 0.008089888
## D    0    0   18 2071    2 0.009564802
## E    0    0    1    6 2338 0.002985075</code></pre>
<p>The top 5 predictors are roll_belt, yaw_belt, magnet_dumbbell_z, pitch_belt, magnet_dumbbell_y Confusion matrix looks ok in training and now the test set from the training dataset will be used to evaluate the model. Accuracy is 0.99 and specificity and sensitivity are both above 0.90. Estimate of error rate is 0.56%</p>
<pre><code>pred.test2 &lt;- predict(results$fit, newdata = test2)
confusionMatrix(pred.test2, test2$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1953   10    0    0    0
##          B    0 1317    9    0    0
##          C    0    1 1187   13    0
##          D    0    0    1 1111    4
##          E    0    0    0    1 1258
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9943         
##                  95% CI : (0.9922, 0.996)
##     No Information Rate : 0.2845         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9928         
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   0.9917   0.9916   0.9876   0.9968
## Specificity            0.9980   0.9984   0.9975   0.9991   0.9998
## Pos Pred Value         0.9949   0.9932   0.9883   0.9955   0.9992
## Neg Pred Value         1.0000   0.9980   0.9982   0.9976   0.9993
## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838
## Detection Rate         0.2845   0.1918   0.1729   0.1618   0.1832
## Detection Prevalence   0.2859   0.1932   0.1749   0.1626   0.1834
## Balanced Accuracy      0.9990   0.9950   0.9946   0.9933   0.9983</code></pre>
<p>With the cv test set we calculate the error rate to be 1- 0.9939 = 0.61%</p>
<p>As observed in the plot after about 20 variables que accuracy is pretty much the same so we will only use the first 20 predictors with a random forest.</p>
</div>

<div id="random-forest">
<h2>
<a id="random-forest" class="anchor" href="#random-forest" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Random forest</h2>
<pre><code>x &lt;- results$optVariables[1:20]
y &lt;- "classe"

mod.rf1 &lt;- train(x = train2[x], y = train2[,y], method = "rf", 
                 trControl = trainControl(method = "cv", 
                                          number = 4))


pred.rf &lt;- predict(mod.rf1$finalModel, newdata = test2[x])
confusionMatrix(pred.rf, test2$classe)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1949   13    0    0    0
##          B    4 1307   12    1    1
##          C    0    8 1178   11    2
##          D    0    0    7 1112    7
##          E    0    0    0    1 1252
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9902          
##                  95% CI : (0.9876, 0.9924)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9877          
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9980   0.9842   0.9841   0.9884   0.9921
## Specificity            0.9974   0.9967   0.9963   0.9976   0.9998
## Pos Pred Value         0.9934   0.9864   0.9825   0.9876   0.9992
## Neg Pred Value         0.9992   0.9962   0.9966   0.9977   0.9982
## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838
## Detection Rate         0.2839   0.1904   0.1716   0.1620   0.1824
## Detection Prevalence   0.2858   0.1930   0.1747   0.1640   0.1825
## Balanced Accuracy      0.9977   0.9905   0.9902   0.9930   0.9959</code></pre>
<p>out of sample error for this random forest is 0.93%</p>
</div>

<div id="knn">
<h2>
<a id="knn" class="anchor" href="#knn" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>KNN</h2>
<pre><code>mod.knn = train(x = train2[x], y = train2[,y], method = "knn",
                 trControl = trainControl(method = "cv", 
                                          number = 4))
pred.knn &lt;- predict(mod.knn, newdata = test2[x])
confusionMatrix(pred.knn, test2$classe)                 </code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1908   64    2    6    5
##          B   28 1155   44    7   25
##          C    7   61 1103   48   30
##          D   10   28   25 1055   26
##          E    0   20   23    9 1176
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9318          
##                  95% CI : (0.9256, 0.9377)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9137          
##  Mcnemar's Test P-Value : 2.897e-08       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9770   0.8697   0.9215   0.9378   0.9319
## Specificity            0.9843   0.9812   0.9742   0.9845   0.9907
## Pos Pred Value         0.9612   0.9174   0.8831   0.9222   0.9577
## Neg Pred Value         0.9908   0.9691   0.9833   0.9878   0.9847
## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838
## Detection Rate         0.2779   0.1682   0.1607   0.1537   0.1713
## Detection Prevalence   0.2891   0.1834   0.1819   0.1666   0.1789
## Balanced Accuracy      0.9806   0.9255   0.9479   0.9611   0.9613</code></pre>
<p>out of sample error for this random forest is 6.76% So the random forest was chosen.</p>
</div>

<div id="final-prediction">
<h1>
<a id="final-prediction" class="anchor" href="#final-prediction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Prediction</h1>
<pre><code>pred.final &lt;- predict(mod.rf1$finalModel, newdata = test[x])</code></pre>
</div>

<p></p>
</div>







<p>
</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/karolur/pml_project">Pml project</a> is maintained by <a href="https://github.com/karolur">karolur</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
