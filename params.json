{"name":"Pml project","tagline":"","body":"\r\n<title>Practical machine learning project</title>\r\n\r\n<body>\r\n\r\n<style type=\"text/css\">\r\n.main-container {\r\n  max-width: 940px;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n}\r\ncode {\r\n  color: inherit;\r\n  background-color: rgba(0, 0, 0, 0.04);\r\n}\r\nimg { \r\n  max-width:100%; \r\n  height: auto; \r\n}\r\n</style>\r\n<div class=\"container-fluid main-container\">\r\n\r\n\r\n<div id=\"header\">\r\n<h1 class=\"title\">Practical machine learning project</h1>\r\n<h4 class=\"author\"><em>Karol</em></h4>\r\n</div>\r\n\r\n\r\n<div id=\"introduction\" class=\"section level2\">\r\n<h2>Introduction</h2>\r\n<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <a href=\"http://groupware.les.inf.puc-rio.br/har\" class=\"uri\">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset). The goal of the project is to predict the manner in which they did their exercises.</p>\r\n</div>\r\n<div id=\"getting-and-cleaning-data\" class=\"section level2\">\r\n<h2>Getting and cleaning data</h2>\r\n<pre class=\"r\"><code>library(reshape2)\r\nlibrary(plyr); library(dplyr)</code></pre>\r\n<pre><code>## \r\n## Attaching package: 'dplyr'\r\n## \r\n## The following objects are masked from 'package:plyr':\r\n## \r\n##     arrange, count, desc, failwith, id, mutate, rename, summarise,\r\n##     summarize\r\n## \r\n## The following object is masked from 'package:stats':\r\n## \r\n##     filter\r\n## \r\n## The following objects are masked from 'package:base':\r\n## \r\n##     intersect, setdiff, setequal, union</code></pre>\r\n<pre class=\"r\"><code>library(caret)</code></pre>\r\n<pre><code>## Loading required package: lattice\r\n## Loading required package: ggplot2</code></pre>\r\n<pre class=\"r\"><code>#URL &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;\r\n#download.file(URL, destfile = &quot;./train_data.csv&quot;, method=&quot;curl&quot;)\r\ntrain &lt;- read.csv(&quot;train_data.csv&quot;)\r\n#URL &lt;- &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv&quot;\r\n#download.file(URL, destfile = &quot;./test_data.csv&quot;, method=&quot;curl&quot;)\r\ntest &lt;- read.csv(&quot;test_data.csv&quot;)</code></pre>\r\n<p>check the general structure of both datasets</p>\r\n<pre class=\"r\"><code>str(train)\r\nstr(test)\r\nnrow(train)\r\nnrow(test)</code></pre>\r\n<p>Because there are some ’’ values, the datasets were re-read. A list of the number of NA per each column was also produced to see if we could eliminate some features, additionally all the columns related to timestamps were deleted as these will not be part of the model.</p>\r\n<pre class=\"r\"><code>train &lt;- read.csv(&quot;train_data.csv&quot;, na.strings=c(&quot;NA&quot;,&quot;&quot;, &quot; &quot;))\r\ntest &lt;- read.csv(&quot;test_data.csv&quot;, na.strings=c(&quot;NA&quot;,&quot;&quot;, &quot; &quot;))\r\n\r\ntrain &lt;- train[, -c(1:7)]\r\n\r\nna_count_train &lt;- sapply(train, function(y) sum(is.na(y)))\r\nna_count_test &lt;- sapply(test, function(y) sum(is.na(y)))</code></pre>\r\n<p>Since no columns have only NA, near zero variance was used converting all values to numeric.</p>\r\n<pre class=\"r\"><code>nzv &lt;- nearZeroVar(train, saveMetrics=TRUE)\r\ntrain &lt;- train[, !nzv$nzv]\r\nisnaCol &lt;- sapply(train, function(y) sum(is.na(y))==0)\r\ntrain &lt;- train[, isnaCol]\r\n\r\nasNumeric &lt;- function(x) as.numeric(as.character(x))\r\nfactorsNumeric &lt;- function(d) {factorCol &lt;- as.vector(sapply(d, is.factor))\r\n                              factorCol[length(factorCol)] = FALSE\r\n                              modifyList(d, lapply(d[, factorCol],   \r\n                                                   asNumeric))}\r\ntrain &lt;- factorsNumeric(train)\r\nfeatures &lt;- names(train[,-length(train)])\r\ntest &lt;- test[,features]</code></pre>\r\n<pre><code>## \r\n##    A    B    C    D    E \r\n## 5580 3797 3422 3216 3607</code></pre>\r\n<p>following <a href=\"http://machinelearningmastery.com/feature-selection-with-the-caret-r-package/\">this guide</a> on feature selection and random forest.</p>\r\n<p>Caret uses recursive feature elimination to select the features and build the model. Random forest was chosen.The algorithm fits the model to all predictors and each predictor is ranked using it’s importance to the model, then for each top ranked predictors the model is refit and assessted until the better performer is retained. Resampling with cross validation is used to incorporate the variation due to the selection at each iteration. A 10 fold cv is used with random forest (rfFuncs)</p>\r\n<pre class=\"r\"><code>set.seed(666)\r\ninTrain = createDataPartition(train$classe, p = 0.65, list = FALSE)\r\ntrain2 = train[inTrain,]\r\ntest2 = train[-inTrain,]\r\n\r\n\r\n# define the control using a random forest selection function\r\ncontrol &lt;- rfeControl(functions=rfFuncs, method=&quot;cv&quot;, number=10)\r\n# run the RFE algorithm\r\nresults &lt;- rfe(train2[,1:52], train2[,53], sizes=c(1:52), rfeControl=control)</code></pre>\r\n<pre><code>## Loading required package: randomForest\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n## \r\n## Attaching package: 'randomForest'\r\n## \r\n## The following object is masked from 'package:dplyr':\r\n## \r\n##     combine</code></pre>\r\n<pre class=\"r\"><code># summarize the results\r\n#print(results)\r\n# list the chosen features\r\npredictors(results)</code></pre>\r\n<pre><code>##  [1] &quot;roll_belt&quot;            &quot;yaw_belt&quot;             &quot;magnet_dumbbell_z&quot;   \r\n##  [4] &quot;pitch_belt&quot;           &quot;magnet_dumbbell_y&quot;    &quot;pitch_forearm&quot;       \r\n##  [7] &quot;accel_dumbbell_y&quot;     &quot;roll_forearm&quot;         &quot;roll_dumbbell&quot;       \r\n## [10] &quot;magnet_dumbbell_x&quot;    &quot;accel_dumbbell_z&quot;     &quot;magnet_forearm_z&quot;    \r\n## [13] &quot;roll_arm&quot;             &quot;gyros_belt_z&quot;         &quot;magnet_belt_z&quot;       \r\n## [16] &quot;magnet_belt_y&quot;        &quot;accel_forearm_x&quot;      &quot;yaw_arm&quot;             \r\n## [19] &quot;magnet_belt_x&quot;        &quot;yaw_dumbbell&quot;         &quot;magnet_forearm_y&quot;    \r\n## [22] &quot;gyros_arm_y&quot;          &quot;accel_forearm_z&quot;      &quot;gyros_dumbbell_y&quot;    \r\n## [25] &quot;accel_belt_z&quot;         &quot;total_accel_dumbbell&quot; &quot;gyros_forearm_y&quot;     \r\n## [28] &quot;magnet_arm_z&quot;         &quot;yaw_forearm&quot;          &quot;gyros_dumbbell_x&quot;    \r\n## [31] &quot;accel_forearm_y&quot;      &quot;gyros_arm_x&quot;          &quot;accel_dumbbell_x&quot;    \r\n## [34] &quot;accel_arm_y&quot;          &quot;gyros_forearm_z&quot;      &quot;magnet_forearm_x&quot;    \r\n## [37] &quot;total_accel_forearm&quot;  &quot;pitch_arm&quot;            &quot;accel_arm_z&quot;         \r\n## [40] &quot;gyros_dumbbell_z&quot;     &quot;accel_arm_x&quot;          &quot;magnet_arm_x&quot;        \r\n## [43] &quot;total_accel_arm&quot;      &quot;gyros_belt_x&quot;         &quot;magnet_arm_y&quot;        \r\n## [46] &quot;accel_belt_x&quot;</code></pre>\r\n<pre class=\"r\"><code>#confusion matrix \r\nprint(results$fit)</code></pre>\r\n<pre><code>## \r\n## Call:\r\n##  randomForest(x = x, y = y, importance = first) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 6\r\n## \r\n##         OOB estimate of  error rate: 0.5%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 3621    5    0    0    1 0.001654260\r\n## B    9 2456    4    0    0 0.005265290\r\n## C    0   15 2207    3    0 0.008089888\r\n## D    0    0   18 2071    2 0.009564802\r\n## E    0    0    1    6 2338 0.002985075</code></pre>\r\n<p>The top 5 predictors are roll_belt, yaw_belt, magnet_dumbbell_z, pitch_belt, magnet_dumbbell_y Confusion matrix looks ok in training and now the test set from the training dataset will be used to evaluate the model. Accuracy is 0.99 and specificity and sensitivity are both above 0.90. Estimate of error rate is 0.56%</p>\r\n<pre class=\"r\"><code>pred.test2 &lt;- predict(results$fit, newdata = test2)\r\nconfusionMatrix(pred.test2, test2$classe)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1953   10    0    0    0\r\n##          B    0 1317    9    0    0\r\n##          C    0    1 1187   13    0\r\n##          D    0    0    1 1111    4\r\n##          E    0    0    0    1 1258\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9943         \r\n##                  95% CI : (0.9922, 0.996)\r\n##     No Information Rate : 0.2845         \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9928         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   0.9917   0.9916   0.9876   0.9968\r\n## Specificity            0.9980   0.9984   0.9975   0.9991   0.9998\r\n## Pos Pred Value         0.9949   0.9932   0.9883   0.9955   0.9992\r\n## Neg Pred Value         1.0000   0.9980   0.9982   0.9976   0.9993\r\n## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2845   0.1918   0.1729   0.1618   0.1832\r\n## Detection Prevalence   0.2859   0.1932   0.1749   0.1626   0.1834\r\n## Balanced Accuracy      0.9990   0.9950   0.9946   0.9933   0.9983</code></pre>\r\n<p>With the cv test set we calculate the error rate to be 1- 0.9939 = 0.61%</p>\r\n<p>As observed in the plot after about 20 variables que accuracy is pretty much the same so we will only use the first 20 predictors with a random forest.</p>\r\n</div>\r\n<div id=\"random-forest\" class=\"section level2\">\r\n<h2>Random forest</h2>\r\n<pre class=\"r\"><code>x &lt;- results$optVariables[1:20]\r\ny &lt;- &quot;classe&quot;\r\n\r\nmod.rf1 &lt;- train(x = train2[x], y = train2[,y], method = &quot;rf&quot;, \r\n                 trControl = trainControl(method = &quot;cv&quot;, \r\n                                          number = 4))\r\n\r\n\r\npred.rf &lt;- predict(mod.rf1$finalModel, newdata = test2[x])\r\nconfusionMatrix(pred.rf, test2$classe)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1949   13    0    0    0\r\n##          B    4 1307   12    1    1\r\n##          C    0    8 1178   11    2\r\n##          D    0    0    7 1112    7\r\n##          E    0    0    0    1 1252\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9902          \r\n##                  95% CI : (0.9876, 0.9924)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9877          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9980   0.9842   0.9841   0.9884   0.9921\r\n## Specificity            0.9974   0.9967   0.9963   0.9976   0.9998\r\n## Pos Pred Value         0.9934   0.9864   0.9825   0.9876   0.9992\r\n## Neg Pred Value         0.9992   0.9962   0.9966   0.9977   0.9982\r\n## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2839   0.1904   0.1716   0.1620   0.1824\r\n## Detection Prevalence   0.2858   0.1930   0.1747   0.1640   0.1825\r\n## Balanced Accuracy      0.9977   0.9905   0.9902   0.9930   0.9959</code></pre>\r\n<p>out of sample error for this random forest is 0.93%</p>\r\n</div>\r\n<div id=\"knn\" class=\"section level2\">\r\n<h2>KNN</h2>\r\n<pre class=\"r\"><code>mod.knn = train(x = train2[x], y = train2[,y], method = &quot;knn&quot;,\r\n                 trControl = trainControl(method = &quot;cv&quot;, \r\n                                          number = 4))\r\npred.knn &lt;- predict(mod.knn, newdata = test2[x])\r\nconfusionMatrix(pred.knn, test2$classe)                 </code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1908   64    2    6    5\r\n##          B   28 1155   44    7   25\r\n##          C    7   61 1103   48   30\r\n##          D   10   28   25 1055   26\r\n##          E    0   20   23    9 1176\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9318          \r\n##                  95% CI : (0.9256, 0.9377)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9137          \r\n##  Mcnemar's Test P-Value : 2.897e-08       \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9770   0.8697   0.9215   0.9378   0.9319\r\n## Specificity            0.9843   0.9812   0.9742   0.9845   0.9907\r\n## Pos Pred Value         0.9612   0.9174   0.8831   0.9222   0.9577\r\n## Neg Pred Value         0.9908   0.9691   0.9833   0.9878   0.9847\r\n## Prevalence             0.2845   0.1934   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2779   0.1682   0.1607   0.1537   0.1713\r\n## Detection Prevalence   0.2891   0.1834   0.1819   0.1666   0.1789\r\n## Balanced Accuracy      0.9806   0.9255   0.9479   0.9611   0.9613</code></pre>\r\n<p>out of sample error for this random forest is 6.76% So the random forest was chosen.</p>\r\n</div>\r\n<div id=\"final-prediction\" class=\"section level1\">\r\n<h1>Final Prediction</h1>\r\n<pre class=\"r\"><code>pred.final &lt;- predict(mod.rf1$finalModel, newdata = test[x])</code></pre>\r\n</div>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}